{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Transfer Learning with Hugging Face Models\n",
    "Part (a): Implement Transfer Learning Using Hugging Face Models\n",
    "For this task, you'll use the Hugging Face library to implement transfer learning with pre-trained models like emilyalsentzer/Bio_ClinicalBERT or the Universal Sentence Encoder. You need to follow these steps:\n",
    "\n",
    "Choose a Model:\n",
    "\n",
    "BERT-based models (Bio_ClinicalBERT): These models are great for text classification tasks, especially medical text, as they are trained specifically on clinical data. The Bio_ClinicalBERT model is a good choice for medical-related tasks.\n",
    "Universal Sentence Encoder (USE): This model is useful for generating sentence embeddings, which can then be used in various downstream tasks like classification, clustering, etc. It is a more generalized model.\n",
    "Load and Implement the Model:\n",
    "\n",
    "You'll load the pre-trained model using the Hugging Face library or TensorFlow Hub.\n",
    "You should preprocess your data (e.g., tokenization for BERT, or text embedding for USE) and fine-tune the model.\n",
    "Custom Class and Functional API:\n",
    "\n",
    "As part of the task, you will implement the model using TensorFlowâ€™s Functional API. This allows flexibility in defining the architecture, especially when wrapping pre-trained models in custom classes.\n",
    "\n",
    "Here is a basic example of how you might integrate Universal Sentence Encoder (USE) in a custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset into a pandas DataFrame\n",
    "data = pd.read_csv('St_Paul_hospital_train.csv')\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the pre-trained Bio_ClinicalBERT tokenizer\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the medical text data\n",
    "def tokenize_texts(texts):\n",
    "    \"\"\"Tokenize the texts and pad/truncate to a fixed length.\"\"\"\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "\n",
    "# Tokenize the medical texts\n",
    "encodings = tokenize_texts(df['medical_text'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert the labels into a TensorFlow tensor\n",
    "labels = tf.convert_to_tensor(df['diagnosis'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split the data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['medical_text'], labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the train and validation texts\n",
    "train_encodings = tokenize_texts(train_texts.tolist())\n",
    "val_encodings = tokenize_texts(val_texts.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create TensorFlow Datasets\n",
    "def create_tf_dataset(encodings, labels):\n",
    "    \"\"\"Create a TensorFlow dataset.\"\"\"\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        {\"input_ids\": encodings['input_ids'], \"attention_mask\": encodings['attention_mask']},\n",
    "        labels\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation datasets\n",
    "train_dataset = create_tf_dataset(train_encodings, train_labels).shuffle(1000).batch(32)\n",
    "val_dataset = create_tf_dataset(val_encodings, val_labels).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build the model using TensorFlow and Bio_ClinicalBERT\n",
    "class BioClinicalBERTLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model):\n",
    "        super(BioClinicalBERTLayer, self).__init__()\n",
    "        self.bert = model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Call function that passes the input through the BERT model.\"\"\"\n",
    "        outputs = self.bert(inputs)\n",
    "        return outputs.last_hidden_state  # Return embeddings from the last layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bio_ClinicalBERT model\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Instantiate the BioClinicalBERT layer\n",
    "bert_layer = BioClinicalBERTLayer(bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer\n",
    "input_text = tf.keras.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "\n",
    "# Pass the input through the Bio_ClinicalBERT layer\n",
    "embedding = bert_layer(input_text)\n",
    "\n",
    "# Use the CLS token to represent the sentence (first token in the output)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(embedding[:, 0, :])  # [CLS] token is at index 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Model(inputs=input_text, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Evaluate the model (optional, with test data)\n",
    "# test_texts = [...]\n",
    "# test_labels = [...]\n",
    "# test_encodings = tokenize_texts(test_texts)\n",
    "# test_labels = tf.convert_to_tensor(test_labels)\n",
    "# test_dataset = create_tf_dataset(test_encodings, test_labels)\n",
    "# model.evaluate(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
