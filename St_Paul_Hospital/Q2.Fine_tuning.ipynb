{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset into a pandas DataFrame\n",
    "df = pd.read_csv('St_Paul_hospital_train.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Format the data as prompt\n",
    "def format_data(row):\n",
    "    return f\"Text: {row['medical_text']} Diagnosis: {row['diagnosis']}\"\n",
    "\n",
    "# Apply to DataFrame\n",
    "df['formatted'] = df.apply(format_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Convert DataFrame to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[['formatted']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Load the GPT2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['formatted'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 5: Split the dataset into train and validation sets\n",
    "train_dataset = tokenized_dataset.train_test_split(test_size=0.1)[\"train\"]\n",
    "val_dataset = tokenized_dataset.train_test_split(test_size=0.1)[\"test\"]\n",
    "\n",
    "# Save the tokenized datasets for fine-tuning\n",
    "train_dataset.save_to_disk('train_dataset')\n",
    "val_dataset.save_to_disk('val_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 6: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medical-diagnosis\",  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=2,  # Batch size for training\n",
    "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir=\"./logs\",  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate every epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Set up the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Fine-tune the model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-gpt2-medical-diagnosis\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt2-medical-diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference with the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_diagnosis(medical_text):\n",
    "    # Format the prompt\n",
    "    prompt = f\"Text: {medical_text} Diagnosis:\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the diagnosis prediction\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=50, num_return_sequences=1)\n",
    "\n",
    "    # Decode the output tokens to text\n",
    "    diagnosis = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return diagnosis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use case\n",
    "medical_text = \"Severe gastrointestinal dysmotility is a newly recognized paraneoplastic syndrome that occurs with small-cell lung carcinoma.\"\n",
    "predicted_diagnosis = predict_diagnosis(medical_text)\n",
    "print(predicted_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Traditional Models (e.g., Logistic Regression, Decision Trees):\n",
    "Strengths:\n",
    "Simple to implement and understand.\n",
    "Faster inference time because they work with manually engineered features rather than large neural networks.\n",
    "Weaknesses:\n",
    "May not capture the complex relationships between words and the medical domain as well as more advanced models.\n",
    "Requires manual feature extraction (TF-IDF, Bag of Words) which may not fully capture the nuances of medical language.\n",
    "Accuracy is usually lower compared to neural models, especially on more complex, unstructured data like medical texts.\n",
    "2. Transfer Learning Model (BERT):\n",
    "Strengths:\n",
    "BERT has a better understanding of the language and context than traditional models. It captures relationships between words more effectively, especially when trained on domain-specific data (medical text).\n",
    "The ability to fine-tune a pre-trained model means faster training and better generalization.\n",
    "Weaknesses:\n",
    "BERT is computationally expensive, both in terms of training and inference time.\n",
    "It requires more memory and resources than traditional models.\n",
    "Performance may plateau after fine-tuning, especially on smaller datasets.\n",
    "3. Fine-tuned GPT Model:\n",
    "Strengths:\n",
    "GPT-2 is a powerful generative model that has shown strong performance in language generation and can be adapted for classification tasks by conditioning the model to predict the next word (or token) that corresponds to the diagnosis.\n",
    "GPT-2 can generate coherent and contextually relevant text, which might be beneficial in understanding complex medical descriptions.\n",
    "After fine-tuning, GPT-2 can be very flexible, providing not only predictions but also human-readable explanations in some cases (depending on how it's fine-tuned).\n",
    "Weaknesses:\n",
    "Like BERT, GPT-2 is computationally intensive and may have slower inference times, particularly for large inputs.\n",
    "Fine-tuning a large model on a relatively small dataset (like the one used in this task) can sometimes lead to overfitting or suboptimal performance if not handled carefully.\n",
    "GPT models are more prone to generating \"hallucinated\" results (i.e., plausible-sounding but incorrect predictions) because they are generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "Traditional models are good for small-scale problems with limited data and are highly efficient in terms of computational resources, but they struggle to capture the complexity of medical text.\n",
    "BERT offers a significant improvement in accuracy and performance by using transfer learning, but it is computationally expensive.\n",
    "GPT-2, while similar to BERT in terms of accuracy, offers additional flexibility by being able to generate explanations and responses. However, it can be prone to overfitting on small datasets and is computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
